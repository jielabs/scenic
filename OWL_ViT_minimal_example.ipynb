{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaiG8Ulc75xc"
   },
   "source": [
    "# OWL-ViT minimal example\n",
    "\n",
    "This Colab shows how to **load a pre-trained OWL-ViT checkpoint** and use it to\n",
    "**get object detection predictions** for an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-Yta1B7rtWu"
   },
   "source": [
    "# Download and install OWL-ViT\n",
    "\n",
    "OWL-ViT is implemented in [Scenic](https://github.com/google-research/scenic). The cell below installs the Scenic codebase from GitHub and imports it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 16039,
     "status": "ok",
     "timestamp": 1652526391946,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "zWF7RkeZ4B_N",
    "outputId": "76775b48-1a9b-45c1-e377-d0c1a3de4562"
   },
   "outputs": [],
   "source": [
    "# !rm -rf *\n",
    "# !rm -rf .config\n",
    "# !rm -rf .git\n",
    "# !git clone https://github.com/google-research/scenic.git .\n",
    "# !python -m pip install -q .\n",
    "# !python -m pip install -r scenic/projects/baselines/clip/requirements.txt\n",
    "# !echo \"Done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 517,
     "status": "ok",
     "timestamp": 1652526392588,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "9MKZb6G3-H92"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import jax\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scenic.projects.owl_vit import models\n",
    "from scenic.projects.owl_vit.configs import clip_b32, clip_l14\n",
    "from scipy.special import expit as sigmoid\n",
    "import skimage\n",
    "from skimage import io as skimage_io\n",
    "from skimage import transform as skimage_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WatINO87evx"
   },
   "source": [
    "# Choose config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1652526392735,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "k4RKu3Vv5k_3"
   },
   "outputs": [],
   "source": [
    "# config = clip_b32.get_config()\n",
    "config = clip_l14.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c12cyRK7oOD"
   },
   "source": [
    "# Load the model and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1652526392887,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "s421Kpp7sXjD"
   },
   "outputs": [],
   "source": [
    "module = models.TextZeroShotDetectionModule(\n",
    "    body_configs=config.model.body,\n",
    "    normalize=config.model.normalize,\n",
    "    box_bias=config.model.box_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://scenic-bucket/owl_vit/checkpoints/clip_vit_l14_d83d374'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.init_from.checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5734,
     "status": "ok",
     "timestamp": 1652526398717,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "WmaY8tQ23nJ3"
   },
   "outputs": [],
   "source": [
    "# checkpoint_path = './clip_vit_b32_b0203fc'\n",
    "checkpoint_path = './clip_vit_l14_d83d374'\n",
    "\n",
    "variables = module.load_variables(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3Knbjoxy2zW"
   },
   "source": [
    "# Prepare image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://static4.depositphotos.com/1004288/279/i/450/depositphotos_2799328-stock-photo-green-peas-served-on-a.jpg\n"
     ]
    }
   ],
   "source": [
    "# filename = os.path.join(skimage.data_dir, 'astronaut.png')\n",
    "filename = './images/dogpark.jpg'\n",
    "filename = './images/trees.jpeg'\n",
    "filename = './images/cows.jpeg'\n",
    "filename = './images/straw.jpeg'\n",
    "filename = './images/peas.jpeg'\n",
    "filename = 'https://static4.depositphotos.com/1004288/279/i/450/depositphotos_2799328-stock-photo-green-peas-served-on-a.jpg'\n",
    "#filename = 'https://miro.medium.com/max/480/1*yuWSTpGIelzw_rlmZyLnlA@2x.jpeg'\n",
    "#filename = 'http://yangjie.org/peas.jpeg'\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "response = requests.get(filename)\n",
    "#print(filename)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "#img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1652526399248,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "99ilV_T2RyNT"
   },
   "outputs": [],
   "source": [
    "# Load example image:\n",
    "# filename = os.path.join(skimage.data_dir, 'astronaut.png')\n",
    "\n",
    "#print(filename)\n",
    "if filename.lower().startswith('http'):\n",
    "    response = requests.get(filename)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    image_uint8 = np.array(image)\n",
    "else:\n",
    "    image_uint8 = skimage_io.imread(filename)\n",
    "\n",
    "image = image_uint8.astype(np.float32) / 255.0\n",
    "\n",
    "# Pad to square with gray pixels on bottom and right:\n",
    "h, w, _ = image.shape\n",
    "size = max(h, w)\n",
    "image_padded = np.pad(\n",
    "    image, ((0, size - h), (0, size - w), (0, 0)), constant_values=0.5)\n",
    "\n",
    "# Resize to model input size:\n",
    "input_image = skimage.transform.resize(\n",
    "    image_padded,\n",
    "    (config.dataset_configs.input_size, config.dataset_configs.input_size),\n",
    "    anti_aliasing=True)\n",
    "\n",
    "#print(input_image.shape)\n",
    "#plt.imshow(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJvG0eaYyplV"
   },
   "source": [
    "# Prepare text queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_query_line = input()\n",
    "text_queries = ['human face', 'rocket', 'nasa badge', 'star-spangled banner']\n",
    "text_queries = ['human', 'dog', 'tree']\n",
    "text_queries = ['house', 'tree']\n",
    "text_queries = ['cow']\n",
    "text_queries = ['green strawberry', 'red strawberry', 'flower', 'tree', 'sky']\n",
    "text_queries = ['pea', 'fork', 'knife', 'plate']\n",
    "text_queries = ['bean']\n",
    "#text_queries = ['cloud', 'flower', 'plant', 'plant community', 'leaf', 'sky']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 11400,
     "status": "ok",
     "timestamp": 1652526410762,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "kSDsqV0UxbtL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1.29M/1.29M [00:00<00:00, 3.80MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "tokenized_queries = np.array([\n",
    "    module.tokenize(q, config.dataset_configs.max_query_length)\n",
    "    for q in text_queries\n",
    "])\n",
    "tokenized_queries_raw = copy.copy(tokenized_queries)\n",
    "#print(tokenized_queries.shape)\n",
    "\n",
    "# Pad tokenized queries to avoid recompilation if number of queries changes:\n",
    "tokenized_queries = np.pad(\n",
    "    tokenized_queries,\n",
    "    pad_width=((0, 100 - len(text_queries)), (0, 0)),\n",
    "    constant_values=0)\n",
    "#print(tokenized_queries.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdR3OpAIzAA0"
   },
   "source": [
    "# Get predictions\n",
    "This will take a minute on the first execution due to model compilation. Subsequent executions will be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 10355,
     "status": "ok",
     "timestamp": 1652526421213,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "M16amaHdzGdK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time: 101.58\n"
     ]
    }
   ],
   "source": [
    "# Note: The model expects a batch dimension.\n",
    "import time\n",
    "\n",
    "start_ts = time.time()\n",
    "predictions = module.apply(\n",
    "    variables,\n",
    "    input_image[None, ...],\n",
    "    tokenized_queries[None, ...],\n",
    "    train=False)\n",
    "\n",
    "# Remove batch dimension and convert to numpy:\n",
    "predictions = jax.tree_map(lambda x: np.array(x[0]), predictions )\n",
    "print('inference time: %.2f' % (time.time() - start_ts))  # 5.31 for b32; 91.45 for l14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# https://i.imgur.com/1IWZX69.jpg\n",
    "\n",
    "import cv2\n",
    "def show(score_threshold, w_threshold, h_threshold, nms_threshold=0.5):\n",
    "    logits = predictions['pred_logits'][..., :len(text_queries)]  # Remove padding.\n",
    "    scores = sigmoid(np.max(logits, axis=-1))\n",
    "    labels = np.argmax(predictions['pred_logits'], axis=-1)\n",
    "    boxes = predictions['pred_boxes']\n",
    "    \n",
    "    rects = [(cx-w/2.0, cy-h/2.0, w, h) for  (cx, cy, w, h) in boxes]\n",
    "#     indicies = nms.boxes(rects, scores, nms_threshold=nms_threshold)\n",
    "    indicies = cv2.dnn.NMSBoxes(rects, scores, score_threshold=score_threshold, nms_threshold=nms_threshold)\n",
    "#    print(len(boxes), score_threshold, nms_threshold, len(indicies))\n",
    "#    print(indicies)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 16))\n",
    "    ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    add_text = True\n",
    "    count = 0\n",
    "    index = 0\n",
    "    actual_indicies = []\n",
    "    for index, values in enumerate(zip(scores, boxes, labels)):\n",
    "      score, box, label = values\n",
    "#       if score < score_threshold:\n",
    "#         continue\n",
    "      if index not in indicies:\n",
    "        continue\n",
    "      cx, cy, w, h = box\n",
    "      if w > w_threshold or h > w_threshold:\n",
    "        continue\n",
    "      count += 1\n",
    "      actual_indicies.append(index)\n",
    "      ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
    "              [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'r')\n",
    "      if add_text:\n",
    "        ax.text(\n",
    "          cx - w / 2 + 0.005,\n",
    "          cy - h / 2 + 0.005,\n",
    "          #f'{text_queries[label]}: {score:1.2f}',\n",
    "          f'{count}',\n",
    "          ha='left',\n",
    "          va='top',\n",
    "          color='red',\n",
    "          bbox={\n",
    "              #'facecolor': 'white',\n",
    "              #'edgecolor': 'red',\n",
    "              'alpha': 0,\n",
    "              'boxstyle': 'square,pad=.3'\n",
    "          })\n",
    "    plt.show()\n",
    "    # return actual_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc8fc11cb024610aaa00c9cd72f8a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='score threshold', max=0.5, step=0.01), FloatSlider(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "#from ipywidgets import interact\n",
    "\n",
    "score_threshold = widgets.FloatSlider(value=0.1, min=0, max=0.5, step=0.01, description='score threshold')\n",
    "nms_threshold = widgets.FloatSlider(value=0.2, min=0, max=1, step=0.01, description='nms threshold')\n",
    "w_threshold = widgets.FloatSlider(value=0.05, min=0, max=1, step=0.01, description='W threshold')\n",
    "h_threshold = widgets.FloatSlider(value=0.05, min=0, max=1, step=0.01, description='H threshold')\n",
    "# nms_threshold = 0.2\n",
    "# w_threshold = 0.05\n",
    "# h_threshold = 0.05\n",
    "\n",
    "\n",
    "# text = widgets.FloatText(disabled=True, description='$Total Count$')\n",
    "\n",
    "# def compute(*ignore):\n",
    "#     indicies = show(score_threshold.value, w_threshold, h_threshold, nms_threshold)\n",
    "#     text.value = str(len(indicies))\n",
    "\n",
    "# score_threshold.observe(compute, 'value')\n",
    "\n",
    "#indicies = show(score_threshold.value, w_threshold, h_threshold, nms_threshold, show=False)\n",
    "#text.value = str(len(indicies))\n",
    "# widgets.VBox([score_threshold, text])\n",
    "\n",
    "_ = widgets.interact(show, score_threshold=score_threshold, w_threshold=w_threshold, h_threshold=h_threshold, nms_threshold=nms_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 480
    },
    "executionInfo": {
     "elapsed": 698,
     "status": "ok",
     "timestamp": 1652526422167,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "ZZPdauOR2ZJ-",
    "outputId": "7e7a717f-78ab-4cca-c4e9-e0e9cbb2433b"
   },
   "outputs": [],
   "source": [
    "score_threshold = 0.05\n",
    "nms_threshold = 0.2\n",
    "w_threshold = 0.05\n",
    "h_threshold = 0.05\n",
    "\n",
    "# indicies = show(score_threshold, w_threshold, h_threshold, nms_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "name": "OWL-ViT minimal example.ipynb",
   "provenance": [
    {
     "file_id": "1kBebGRuMcABXiprw6IEAxpbKAXNO4EOQ",
     "timestamp": 1651575080312
    },
    {
     "file_id": "https://github.com/google-research/scenic/blob/main/scenic/common_lib/colabs/scenic_playground.ipynb",
     "timestamp": 1650960476931
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
